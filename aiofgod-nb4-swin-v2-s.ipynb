{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":117194,"databundleVersionId":14005367,"sourceType":"competition"},{"sourceId":604292,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":453129,"modelId":469425},{"sourceId":604407,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":453233,"modelId":469532}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport copy\nimport random\nimport numpy as np\nfrom tqdm import tqdm\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import confusion_matrix\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Subset\nfrom torchvision import datasets, transforms, models\n\ndata_dir = \"/kaggle/input/AI-OF-GOD-4/aog_data/train\"\nnum_classes = 9\nnum_epochs = 10\nbatch_size = 16\nlearning_rate = 1e-4\nnum_folds = 3\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nseed = 42\ntorch.manual_seed(seed)\nnp.random.seed(seed)\nrandom.seed(seed)\n\ntrain_transform = transforms.Compose([\n    transforms.Resize((384, 384)),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(10),\n    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.05),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                         std=[0.229, 0.224, 0.225]),\n])\n\ndataset = datasets.ImageFolder(root=data_dir, transform=train_transform)\nclass_names = dataset.classes\nlabels = [label for _, label in dataset.samples]\n\ndef create_model(model_name=\"swin_v2_s\", num_classes=num_classes, pretrained=True):\n    if model_name == \"regnet_y_16gf\":\n        model = models.regnet_y_16gf(weights=models.RegNet_Y_16GF_Weights.IMAGENET1K_V2 if pretrained else None)\n        in_features = model.fc.in_features\n        model.fc = nn.Linear(in_features, num_classes)\n    elif model_name == \"swin_v2_s\":\n        model = models.swin_v2_s(weights=models.Swin_V2_S_Weights.IMAGENET1K_V1 if pretrained else None)\n        in_features = model.head.in_features\n        model.head = nn.Linear(in_features, num_classes)\n    else:\n        raise ValueError(\"Unknown model name. Use 'regnet_y_16gf' or 'swin_v2_s'.\")\n    return model.to(device)\n\ndef train_one_epoch(model, loader, criterion, optimizer):\n    model.train()\n    running_loss, correct, total = 0.0, 0, 0\n    for inputs, labels in tqdm(loader, leave=False):\n        inputs, labels = inputs.to(device), labels.to(device)\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item() * inputs.size(0)\n        _, preds = torch.max(outputs, 1)\n        correct += (preds == labels).sum().item()\n        total += labels.size(0)\n    epoch_loss = running_loss / len(loader.dataset)\n    epoch_acc = correct / total\n    return epoch_loss, epoch_acc\n\ndef validate(model, loader, criterion, class_names):\n    model.eval()\n    running_loss, correct, total = 0.0, 0, 0\n    all_preds, all_labels = [], []\n    with torch.no_grad():\n        for inputs, labels in tqdm(loader, leave=False):\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            running_loss += loss.item() * inputs.size(0)\n            _, preds = torch.max(outputs, 1)\n            correct += (preds == labels).sum().item()\n            total += labels.size(0)\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n    val_loss = running_loss / len(loader.dataset)\n    val_acc = correct / total\n    cm = confusion_matrix(all_labels, all_preds, labels=list(range(len(class_names))))\n    per_class_acc = cm.diagonal() / cm.sum(axis=1)\n    per_class_acc_dict = {cls: acc for cls, acc in zip(class_names, per_class_acc)}\n    return val_loss, val_acc, per_class_acc_dict\n\nskf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=seed)\n\nfor fold, (train_idx, val_idx) in enumerate(skf.split(np.zeros(len(labels)), labels)):\n    print(f\"\\n========== Fold {fold + 1}/{num_folds} ==========\")\n    train_subset = Subset(dataset, train_idx)\n    val_subset = Subset(dataset, val_idx)\n    train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True, num_workers=2)\n    val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False, num_workers=2)\n    model = create_model(model_name=\"swin_v2_s\")\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n    best_val_acc = 0.0\n    best_model_wts = copy.deepcopy(model.state_dict())\n    for epoch in range(num_epochs):\n        print(f\"\\nEpoch [{epoch+1}/{num_epochs}] â€” Fold {fold+1}\")\n        train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer)\n        val_loss, val_acc, per_class_acc = validate(model, val_loader, criterion, class_names)\n        scheduler.step()\n        print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n        print(f\"Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc:.4f}\")\n        print(\"Per-class accuracies:\")\n        for cls, acc in per_class_acc.items():\n            print(f\"  {cls:25s} : {acc*100:.2f}%\")\n        if val_acc > best_val_acc:\n            best_val_acc = val_acc\n            best_model_wts = copy.deepcopy(model.state_dict())\n            model_path = f\"best_model_fold{fold+1}_epoch{epoch+1}.pth\"\n            torch.save(best_model_wts, model_path)\n            print(f\"âœ… Model improved & saved: {model_path}\")\n    print(f\"Best Val Accuracy for Fold {fold+1}: {best_val_acc:.4f}\")\n\nprint(\"\\nðŸŽ‰ Training Complete for All Folds!\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport copy\nimport random\nimport numpy as np\nfrom tqdm import tqdm\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import confusion_matrix\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Subset\nfrom torchvision import datasets, transforms, models\n\ndata_dir = \"/kaggle/input/AI-OF-GOD-4/aog_data/train\"\nnum_classes = 9\nnum_epochs = 10\nbatch_size = 16\nlearning_rate = 1e-4\nnum_folds = 3\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nseed = 42\ntorch.manual_seed(seed)\nnp.random.seed(seed)\nrandom.seed(seed)\n\ntrain_transform = transforms.Compose([\n    transforms.Resize((384, 384)),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(10),\n    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.05),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                         std=[0.229, 0.224, 0.225]),\n])\n\ndataset = datasets.ImageFolder(root=data_dir, transform=train_transform)\nclass_names = dataset.classes\nlabels = [label for _, label in dataset.samples]\n\ndef create_model(model_name=\"swin_v2_s\", num_classes=num_classes, pretrained=True):\n    if model_name == \"regnet_y_16gf\":\n        model = models.regnet_y_16gf(weights=models.RegNet_Y_16GF_Weights.IMAGENET1K_V2 if pretrained else None)\n        in_features = model.fc.in_features\n        model.fc = nn.Linear(in_features, num_classes)\n    elif model_name == \"swin_v2_s\":\n        model = models.swin_v2_s(weights=models.Swin_V2_S_Weights.IMAGENET1K_V1 if pretrained else None)\n        in_features = model.head.in_features\n        model.head = nn.Linear(in_features, num_classes)\n    else:\n        raise ValueError(\"Unknown model name. Use 'regnet_y_16gf' or 'swin_v2_s'.\")\n    return model.to(device)\n\ndef train_one_epoch(model, loader, criterion, optimizer):\n    model.train()\n    running_loss, correct, total = 0.0, 0, 0\n    for inputs, labels in tqdm(loader, leave=False):\n        inputs, labels = inputs.to(device), labels.to(device)\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item() * inputs.size(0)\n        _, preds = torch.max(outputs, 1)\n        correct += (preds == labels).sum().item()\n        total += labels.size(0)\n    epoch_loss = running_loss / len(loader.dataset)\n    epoch_acc = correct / total\n    return epoch_loss, epoch_acc\n\ndef validate(model, loader, criterion, class_names):\n    model.eval()\n    running_loss, correct, total = 0.0, 0, 0\n    all_preds, all_labels = [], []\n    with torch.no_grad():\n        for inputs, labels in tqdm(loader, leave=False):\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            running_loss += loss.item() * inputs.size(0)\n            _, preds = torch.max(outputs, 1)\n            correct += (preds == labels).sum().item()\n            total += labels.size(0)\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n    val_loss = running_loss / len(loader.dataset)\n    val_acc = correct / total\n    cm = confusion_matrix(all_labels, all_preds, labels=list(range(len(class_names))))\n    per_class_acc = cm.diagonal() / cm.sum(axis=1)\n    per_class_acc_dict = {cls: acc for cls, acc in zip(class_names, per_class_acc)}\n    return val_loss, val_acc, per_class_acc_dict\n\nskf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=seed)\nfold2_checkpoint = \"/kaggle/input/swin_v2_fold2_epoch3/pytorch/default/1/best_model_fold2_epoch3.pth\"\n\nfor fold, (train_idx, val_idx) in enumerate(skf.split(np.zeros(len(labels)), labels)):\n    if fold == 0:\n        print(\"\\n========== Skipping Fold 1 ==========\")\n        continue\n\n    print(f\"\\n========== Fold {fold + 1}/{num_folds} ==========\")\n    train_subset = Subset(dataset, train_idx)\n    val_subset = Subset(dataset, val_idx)\n    train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True, num_workers=2)\n    val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False, num_workers=2)\n\n    model = create_model(model_name=\"swin_v2_s\")\n    start_epoch = 0\n\n    if fold == 1:\n        state_dict = torch.load(fold2_checkpoint, map_location=device)\n        model.load_state_dict(state_dict)\n        start_epoch = 3\n        print(f\"âœ… Loaded checkpoint for Fold 2, resuming from epoch {start_epoch}\")\n\n    if torch.cuda.device_count() > 1:\n        model = nn.DataParallel(model)\n        print(f\"âœ… Using {torch.cuda.device_count()} GPUs\")\n\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n\n    best_val_acc = 0.0\n    best_model_wts = copy.deepcopy(model.state_dict())\n\n    for epoch in range(start_epoch, num_epochs):\n        print(f\"\\nEpoch [{epoch+1}/{num_epochs}] â€” Fold {fold+1}\")\n        train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer)\n        val_loss, val_acc, per_class_acc = validate(model, val_loader, criterion, class_names)\n        scheduler.step()\n        print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n        print(f\"Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc:.4f}\")\n        print(\"Per-class accuracies:\")\n        for cls, acc in per_class_acc.items():\n            print(f\"  {cls:25s} : {acc*100:.2f}%\")\n        if val_acc > best_val_acc:\n            best_val_acc = val_acc\n            best_model_wts = copy.deepcopy(model.state_dict())\n            model_path = f\"best_model_fold{fold+1}_epoch{epoch+1}.pth\"\n            torch.save(best_model_wts, model_path)\n            print(f\"âœ… Model improved & saved: {model_path}\")\n\n    print(f\"Best Val Accuracy for Fold {fold+1}: {best_val_acc:.4f}\")\n\nprint(\"\\nðŸŽ‰ Training Complete for All Folds!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T09:39:24.333339Z","iopub.execute_input":"2025-10-11T09:39:24.334133Z","iopub.status.idle":"2025-10-11T11:56:21.912581Z","shell.execute_reply.started":"2025-10-11T09:39:24.334108Z","shell.execute_reply":"2025-10-11T11:56:21.911660Z"}},"outputs":[{"name":"stdout","text":"\n========== Skipping Fold 1 ==========\n\n========== Fold 2/3 ==========\nâœ… Loaded checkpoint for Fold 2, resuming from epoch 3\nâœ… Using 2 GPUs\n\nEpoch [4/10] â€” Fold 2\n","output_type":"stream"},{"name":"stderr","text":"                                                 \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.2095 | Train Acc: 0.9338\nVal Loss:   0.2517 | Val Acc:   0.9182\nPer-class accuracies:\n  0                         : 88.21%\n  1                         : 96.02%\n  2                         : 92.47%\n  3                         : 98.73%\n  4                         : 84.58%\n  5                         : 92.43%\n  6                         : 99.57%\n  7                         : 94.96%\n  8                         : 74.27%\nâœ… Model improved & saved: best_model_fold2_epoch4.pth\n\nEpoch [5/10] â€” Fold 2\n","output_type":"stream"},{"name":"stderr","text":"                                                 \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1464 | Train Acc: 0.9513\nVal Loss:   0.1857 | Val Acc:   0.9395\nPer-class accuracies:\n  0                         : 90.87%\n  1                         : 93.23%\n  2                         : 98.12%\n  3                         : 97.89%\n  4                         : 85.90%\n  5                         : 90.04%\n  6                         : 90.04%\n  7                         : 87.98%\n  8                         : 79.25%\nâœ… Model improved & saved: best_model_fold2_epoch5.pth\n\nEpoch [6/10] â€” Fold 2\n","output_type":"stream"},{"name":"stderr","text":"                                                 \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1320 | Train Acc: 0.9573\nVal Loss:   0.1847 | Val Acc:   0.9405\nPer-class accuracies:\n  0                         : 82.89%\n  1                         : 90.44%\n  2                         : 96.71%\n  3                         : 91.98%\n  4                         : 91.19%\n  5                         : 93.23%\n  6                         : 94.81%\n  7                         : 93.80%\n  8                         : 91.70%\nâœ… Model improved & saved: best_model_fold2_epoch6.pth\n\nEpoch [7/10] â€” Fold 2\n","output_type":"stream"},{"name":"stderr","text":"                                                 \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0891 | Train Acc: 0.9706\nVal Loss:   0.1618 | Val Acc:   0.9530\nPer-class accuracies:\n  0                         : 89.73%\n  1                         : 97.21%\n  2                         : 97.37%\n  3                         : 90.72%\n  4                         : 88.99%\n  5                         : 98.80%\n  6                         : 98.27%\n  7                         : 93.80%\n  8                         : 86.72%\nâœ… Model improved & saved: best_model_fold2_epoch7.pth\n\nEpoch [8/10] â€” Fold 2\n","output_type":"stream"},{"name":"stderr","text":"                                                 \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0694 | Train Acc: 0.9775\nVal Loss:   0.1799 | Val Acc:   0.9550\nPer-class accuracies:\n  0                         : 91.63%\n  1                         : 93.63%\n  2                         : 97.93%\n  3                         : 96.62%\n  4                         : 92.07%\n  5                         : 95.62%\n  6                         : 98.27%\n  7                         : 93.41%\n  8                         : 81.74%\nâœ… Model improved & saved: best_model_fold2_epoch8.pth\n\nEpoch [9/10] â€” Fold 2\n","output_type":"stream"},{"name":"stderr","text":"                                                 \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0406 | Train Acc: 0.9878\nVal Loss:   0.1410 | Val Acc:   0.9677\nPer-class accuracies:\n  0                         : 93.92%\n  1                         : 97.61%\n  2                         : 98.02%\n  3                         : 97.47%\n  4                         : 96.04%\n  5                         : 96.81%\n  6                         : 96.97%\n  7                         : 97.67%\n  8                         : 86.72%\nâœ… Model improved & saved: best_model_fold2_epoch9.pth\n\nEpoch [10/10] â€” Fold 2\n","output_type":"stream"},{"name":"stderr","text":"                                                 \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0286 | Train Acc: 0.9902\nVal Loss:   0.1390 | Val Acc:   0.9667\nPer-class accuracies:\n  0                         : 84.41%\n  1                         : 98.80%\n  2                         : 98.35%\n  3                         : 97.89%\n  4                         : 95.15%\n  5                         : 96.81%\n  6                         : 97.84%\n  7                         : 97.67%\n  8                         : 90.87%\nBest Val Accuracy for Fold 2: 0.9677\n\n========== Fold 3/3 ==========\nâœ… Using 2 GPUs\n\nEpoch [1/10] â€” Fold 3\n","output_type":"stream"},{"name":"stderr","text":"                                                 \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.7461 | Train Acc: 0.7520\nVal Loss:   0.3745 | Val Acc:   0.8869\nPer-class accuracies:\n  0                         : 70.23%\n  1                         : 82.14%\n  2                         : 91.01%\n  3                         : 94.54%\n  4                         : 94.71%\n  5                         : 88.84%\n  6                         : 97.39%\n  7                         : 99.23%\n  8                         : 63.90%\nâœ… Model improved & saved: best_model_fold3_epoch1.pth\n\nEpoch [2/10] â€” Fold 3\n","output_type":"stream"},{"name":"stderr","text":"                                                 \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.3046 | Train Acc: 0.9015\nVal Loss:   0.2632 | Val Acc:   0.9094\nPer-class accuracies:\n  0                         : 54.58%\n  1                         : 80.95%\n  2                         : 95.53%\n  3                         : 97.06%\n  4                         : 93.83%\n  5                         : 84.06%\n  6                         : 95.22%\n  7                         : 95.75%\n  8                         : 89.63%\nâœ… Model improved & saved: best_model_fold3_epoch2.pth\n\nEpoch [3/10] â€” Fold 3\n","output_type":"stream"},{"name":"stderr","text":"                                                 \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.2162 | Train Acc: 0.9309\nVal Loss:   0.1966 | Val Acc:   0.9390\nPer-class accuracies:\n  0                         : 71.76%\n  1                         : 92.86%\n  2                         : 95.67%\n  3                         : 95.38%\n  4                         : 97.36%\n  5                         : 96.81%\n  6                         : 95.65%\n  7                         : 98.84%\n  8                         : 88.80%\nâœ… Model improved & saved: best_model_fold3_epoch3.pth\n\nEpoch [4/10] â€” Fold 3\n","output_type":"stream"},{"name":"stderr","text":"                                                 \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1449 | Train Acc: 0.9541\nVal Loss:   0.2437 | Val Acc:   0.9266\nPer-class accuracies:\n  0                         : 87.79%\n  1                         : 78.17%\n  2                         : 94.92%\n  3                         : 98.74%\n  4                         : 96.48%\n  5                         : 97.21%\n  6                         : 96.52%\n  7                         : 77.61%\n  8                         : 91.29%\n\nEpoch [5/10] â€” Fold 3\n","output_type":"stream"},{"name":"stderr","text":"                                                 \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1058 | Train Acc: 0.9645\nVal Loss:   0.1716 | Val Acc:   0.9461\nPer-class accuracies:\n  0                         : 84.35%\n  1                         : 88.89%\n  2                         : 98.16%\n  3                         : 97.90%\n  4                         : 87.22%\n  5                         : 90.84%\n  6                         : 95.22%\n  7                         : 94.59%\n  8                         : 87.55%\nâœ… Model improved & saved: best_model_fold3_epoch5.pth\n\nEpoch [6/10] â€” Fold 3\n","output_type":"stream"},{"name":"stderr","text":"                                                 \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0641 | Train Acc: 0.9804\nVal Loss:   0.1690 | Val Acc:   0.9542\nPer-class accuracies:\n  0                         : 91.98%\n  1                         : 87.30%\n  2                         : 98.45%\n  3                         : 94.54%\n  4                         : 94.27%\n  5                         : 94.82%\n  6                         : 94.35%\n  7                         : 92.66%\n  8                         : 87.55%\nâœ… Model improved & saved: best_model_fold3_epoch6.pth\n\nEpoch [7/10] â€” Fold 3\n","output_type":"stream"},{"name":"stderr","text":"                                                 \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0401 | Train Acc: 0.9879\nVal Loss:   0.1414 | Val Acc:   0.9633\nPer-class accuracies:\n  0                         : 92.75%\n  1                         : 96.43%\n  2                         : 96.52%\n  3                         : 97.90%\n  4                         : 98.68%\n  5                         : 95.62%\n  6                         : 98.70%\n  7                         : 96.91%\n  8                         : 92.53%\nâœ… Model improved & saved: best_model_fold3_epoch7.pth\n\nEpoch [8/10] â€” Fold 3\n","output_type":"stream"},{"name":"stderr","text":"                                                 \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0234 | Train Acc: 0.9923\nVal Loss:   0.1326 | Val Acc:   0.9682\nPer-class accuracies:\n  0                         : 94.27%\n  1                         : 93.65%\n  2                         : 98.02%\n  3                         : 98.32%\n  4                         : 98.24%\n  5                         : 96.81%\n  6                         : 96.96%\n  7                         : 97.30%\n  8                         : 88.80%\nâœ… Model improved & saved: best_model_fold3_epoch8.pth\n\nEpoch [9/10] â€” Fold 3\n","output_type":"stream"},{"name":"stderr","text":"                                                 \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0173 | Train Acc: 0.9941\nVal Loss:   0.1231 | Val Acc:   0.9721\nPer-class accuracies:\n  0                         : 92.75%\n  1                         : 96.03%\n  2                         : 98.16%\n  3                         : 99.16%\n  4                         : 97.80%\n  5                         : 97.61%\n  6                         : 96.96%\n  7                         : 98.07%\n  8                         : 91.29%\nâœ… Model improved & saved: best_model_fold3_epoch9.pth\n\nEpoch [10/10] â€” Fold 3\n","output_type":"stream"},{"name":"stderr","text":"                                                 \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0127 | Train Acc: 0.9967\nVal Loss:   0.1126 | Val Acc:   0.9745\nPer-class accuracies:\n  0                         : 91.98%\n  1                         : 96.03%\n  2                         : 98.45%\n  3                         : 99.58%\n  4                         : 98.68%\n  5                         : 97.61%\n  6                         : 97.83%\n  7                         : 98.46%\n  8                         : 91.29%\nâœ… Model improved & saved: best_model_fold3_epoch10.pth\nBest Val Accuracy for Fold 3: 0.9745\n\nðŸŽ‰ Training Complete for All Folds!\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms, models\nimport numpy as np\nfrom tqdm import tqdm\nimport pandas as pd\n\ntest_dir = \"/kaggle/input/AI-OF-GOD-4/aog_data/test/images\"\nnum_classes = 9\nbatch_size = 16\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nmodel_paths = [\n    \"/kaggle/input/swin-v2_s-fold-1/pytorch/default/1/best_model_fold1_epoch10.pth\",\n    \"/kaggle/working/best_model_fold2_epoch9.pth\",\n    \"/kaggle/working/best_model_fold3_epoch9.pth\",\n]\n\ntest_transform = transforms.Compose([\n    transforms.Resize((384, 384)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                         std=[0.229, 0.224, 0.225]),\n])\n\ntest_dataset = datasets.ImageFolder(root=os.path.dirname(test_dir), transform=test_transform)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n\ndef create_swin_v2_s(num_classes, pretrained=False):\n    model = models.swin_v2_s(weights=models.Swin_V2_S_Weights.IMAGENET1K_V1 if pretrained else None)\n    in_features = model.head.in_features\n    model.head = nn.Linear(in_features, num_classes)\n    return model\n\nmodels_list = []\nfor i, path in enumerate(model_paths):\n    print(f\"\\nðŸ”¹ Loading model {i+1} from {path}\")\n    model = create_swin_v2_s(num_classes=num_classes)\n    state_dict = torch.load(path, map_location=device)\n    if any(k.startswith(\"module.\") for k in state_dict.keys()):\n        state_dict = {k.replace(\"module.\", \"\"): v for k, v in state_dict.items()}\n    model.load_state_dict(state_dict, strict=True)\n    model.to(device)\n    if torch.cuda.device_count() > 1:\n        model = nn.DataParallel(model)\n        print(f\"âœ… Using {torch.cuda.device_count()} GPUs\")\n    model.eval()\n    models_list.append(model)\n\nall_outputs = []\nall_preds = []\n\nprint(\"\\nðŸš€ Running Inference...\")\nwith torch.no_grad():\n    for inputs, _ in tqdm(test_loader):\n        inputs = inputs.to(device)\n        outputs_per_model = []\n        for model in models_list:\n            outputs = model(inputs)\n            outputs_per_model.append(torch.softmax(outputs, dim=1))\n        stacked_outputs = torch.stack(outputs_per_model)\n        avg_outputs = torch.mean(stacked_outputs, dim=0)\n        all_outputs.append(avg_outputs.cpu().numpy())\n        preds_each_model = [torch.argmax(o, dim=1).cpu().numpy() for o in outputs_per_model]\n        all_preds.append(preds_each_model)\n\navg_outputs = np.concatenate(all_outputs, axis=0)\nfinal_preds = np.argmax(avg_outputs, axis=1)\nmodel_preds = [np.concatenate([batch[i] for batch in all_preds]) for i in range(len(model_paths))]\n\nos.makedirs(\"/kaggle/working/predictions\", exist_ok=True)\nfilenames = [os.path.basename(p[0]) for p in test_dataset.samples]\n\nfor i, preds in enumerate(model_preds):\n    df = pd.DataFrame({\"filename\": filenames, \"pred_class\": preds})\n    df.to_csv(f\"/kaggle/working/predictions/model{i+1}_preds.csv\", index=False)\n    print(f\"âœ… Saved: model{i+1}_preds.csv\")\n\nensemble_df = pd.DataFrame({\"filename\": filenames, \"ensemble_pred_class\": final_preds})\nensemble_df.to_csv(\"/kaggle/working/predictions/ensemble_preds.csv\", index=False)\nprint(\"âœ… Saved: ensemble_preds.csv\")\n\nprint(\"\\nðŸŽ‰ Inference Complete! All results are in /kaggle/working/predictions\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T12:15:05.102825Z","iopub.execute_input":"2025-10-11T12:15:05.103141Z","iopub.status.idle":"2025-10-11T12:19:36.378043Z","shell.execute_reply.started":"2025-10-11T12:15:05.103121Z","shell.execute_reply":"2025-10-11T12:19:36.377141Z"}},"outputs":[{"name":"stdout","text":"\nðŸ”¹ Loading model 1 from /kaggle/input/swin-v2_s-fold-1/pytorch/default/1/best_model_fold1_epoch10.pth\nâœ… Using 2 GPUs\n\nðŸ”¹ Loading model 2 from /kaggle/working/best_model_fold2_epoch9.pth\nâœ… Using 2 GPUs\n\nðŸ”¹ Loading model 3 from /kaggle/working/best_model_fold3_epoch9.pth\nâœ… Using 2 GPUs\n\nðŸš€ Running Inference...\n","output_type":"stream"},{"name":"stderr","text":"100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 329/329 [04:25<00:00,  1.24it/s]","output_type":"stream"},{"name":"stdout","text":"âœ… Saved: model1_preds.csv\nâœ… Saved: model2_preds.csv\nâœ… Saved: model3_preds.csv\nâœ… Saved: ensemble_preds.csv\n\nðŸŽ‰ Inference Complete! All results are in /kaggle/working/predictions\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}